{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5a - Calculation of $\\pi$\n",
    "\n",
    "### Goals:\n",
    "- compare different integration methods for $\\pi$\n",
    "- determine the behaviour of the error as a function of the number of samplings for different methods\n",
    "- study the autocorrelation between consecutive samplings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throw and count\n",
    "This might be the most straightforward technique to calculate π. You throw the darts randomly at the square dart board a great many times, and then count the number of darts which fell inside the circle. This gives you an estimate of the ratio of the circle area (which is π related) to the square board area (suppose there is no dart outside the board). Mathematically, we can resort to a function $f (x)$ which defines a circle \n",
    "$$f(x, y)=\\left\\{\\begin{array}{ll}\n",
    "1, & \\sqrt{x^2+y^2} \\leq 1 \\\\\n",
    "0, & \\text { otherwise }\n",
    "\\end{array}\\right.$$\n",
    "where $x$ and $y$ are randomly distributed in the region [0, 1]. Then the evaluation of π is equivalent to the summation\n",
    "$$ \\pi=\\frac{4}{N} \\sum_{i=1}^N f_i(x, y) $$\n",
    "where $N$ is the total number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct integration\n",
    "\n",
    "The value of $\\pi$ can be expressed as\n",
    "$$ \\pi  = 4 \\int_0^2 \\sqrt{1-x^2}\\, \\text{d}x$$\n",
    "or alternatively it can be estimated using the quantity\n",
    "$$A=\\int_0^1\\frac{1}{1+x^2}\\,\\text{d}x.$$\n",
    "A basic Monte Carlo integration suggests that the estimate of an integral $A=\\int_0^1 f(x)\\,\\text{d}x$ can be obtained by dividing the region [0,1] into $N$ segments, followed by a summation\n",
    "$$A=\\frac{1}{N} \\sum_{i=1}^N f\\left(x_i\\right) \\pm \\frac{1}{\\sqrt{N}} \\sqrt{\\left\\langle f_i^2\\right\\rangle-\\left\\langle f_i\\right\\rangle^2}$$\n",
    "where $\\left\\langle f_i^2\\right\\rangle=\\sum f^2\\left(x_i\\right) / N,$ and  $\\left\\langle f_i\\right\\rangle=\\sum_i f\\left(x_i\\right) / N.$ The error estimate of basic Monte Carlo integration therefore scales with $1/\\sqrt{N}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling \n",
    "Very often, we find direct Monte Carlo integration is far less efficient than other integration techniques (e.g. trapezoidal or Simpson's rule) at least for one dimensional systems.\n",
    "This is already quite evident from the last section.\n",
    "The basic integration scheme works better when the integrand $f(x)$ is smoother.\n",
    "It is thus a natural idea to replace $f(x)$ by a smoother function if it's possible.\n",
    "In importance sampling method, one introduces a weight function $w(x)$ so that the integral becomes $$ A = \\int_0^1 \\frac{f(x)}{w(x)}w(x)\\,\\text{d}x.$$\n",
    "If the behavior of $w(x)$ is similar to that of $f(x),$ the new integrand $g(x) = \\frac{f(x)}{g(x)}$ shows much smaller variation than $f(x)$ and the Monte Carlo inegration will be more reliable for smaller $N$s.\n",
    "\n",
    "The central issue of the scheme is to find an appropriate weight function.\n",
    "For our task here, con can start with a trial function $w(x) = (4-2x)/3,$ which is properly normalised for $x\\in [0,1].$\n",
    "We then rewrite the original integral into $$ \\tilde{A}=\\int_0^1 g(x) w(x)\\, \\text{d} x=\\int_0^1 \\frac{3}{\\left[1+x(y)^2\\right][4-2 x(y)]}\\,\\text{d} y $$ where $x=2-\\sqrt{4-3y}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsize = 10000\n",
    "verbose = 1\n",
    "#various Monte Carlo methods\n",
    "\n",
    "n = nsize\n",
    "\n",
    "task = mc.montecarlo(n, verbose=verbose)\n",
    "task.sampling()\n",
    "task.direct()\n",
    "task.importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to monitor the error $\\frac{1}{\\sqrt{N}} \\sqrt{\\left\\langle f_i^2\\right\\rangle-\\left\\langle f_i\\right\\rangle^2}$ with respect to the number of sampling points $N.$\n",
    "We can loop the direct and importance sampling schemes and save the output to a variable (`data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error estimate\n",
    "n_init = 1000\n",
    "n_final = 10000\n",
    "n_step = 200\n",
    "data=np.zeros((int((n_final-n_init)/n_step),5))\n",
    "for i in range(n_init, n_final, n_step):\n",
    "   n = i\n",
    "   task = mc.montecarlo(n,verbose=0)\n",
    "   task.direct()\n",
    "   task.importance()\n",
    "   data[int((n-n_init)/n_step), 0] = n\n",
    "   data[int((n-n_init)/n_step), 1] = task.pi_1 # direct estimate\n",
    "   data[int((n-n_init)/n_step), 2] = task.ds_1 # direct error\n",
    "   data[int((n-n_init)/n_step), 3] = task.pi_2 # importance estimate\n",
    "   data[int((n-n_init)/n_step), 4] = task.ds_2 # importance error\n",
    "#    print('{0:10} {1:.10f} {2:.10f}'.format(n, task.ds_1, task.ds_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the standard deviation estimates of the error and check that they behave as $1/\\sqrt{N}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>TODO:</b> Plot the standard deviation estimates of the error and verify their behaviour using curve_fit </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "plot the error estimate \n",
    "and fit curve to the error estimate a*sqrt(1/n) + b\n",
    "\"\"\"\n",
    "fitfun = lambda x, a, b: a*np.sqrt(1/x) + b\n",
    "\n",
    "\n",
    "# direct sampling\n",
    "#plt.plot(... # plot the error estimate\n",
    "#popt0, pcov = curve_fit(... # fit the curve\n",
    "#plt.plot(... # plot the fitted curve\n",
    "\n",
    "# importance sampling\n",
    "# ...;\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also be interested in the absolute error $|\\pi -4A|.$\n",
    "\n",
    "You can plot the absolute error vs $N$ and see how direct and importance sampling integration perform for small $N\\text{s}$.\n",
    "\n",
    "Get a good estimate of $\\pi$ from `np.pi`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>TODO:</b> Plot the absolute error for direct sampling and importance sampling</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute and plot absolute error \"\"\"\n",
    "\n",
    "# direct sampling\n",
    "plt.figure()\n",
    "# plt.plot(... , ... , label='direct',marker='x', ls='-',\n",
    "#          lw=.7,) <-------------- FILL IN\n",
    "\n",
    "# importance sampling\n",
    "# plt.plot( ... , ... ,  label='importance',marker='1', ls='-',\n",
    "#          lw='.7',) <-------------- FILL IN\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('absolute error $|\\pi-4A|$')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis algorithm\n",
    "\n",
    "In the last few examples, random numbers were drawn from uniform distributions in the sampling region.\n",
    "As we have discussed in the importance sampling, this is not very efficient when the variation of the integrand is large.\n",
    "In fact, both direct sampling and basic importance sampling result in fairly large error estimates for the $\\pi$ evaluation.\n",
    "\n",
    "Metropolis proposed in 1953 an alternative method to randomly sample the points based on the Boltzmann distribution.\n",
    "Much alike the central idea in the importance sampling, we sample those points more often with higher probabilities.\n",
    "But what makes Metropolis algorithm truly unique and interesting is that the sampling procedure now follows along a *Markov chain*, so that the new state depends only on the previous state.\n",
    "In other words,the points in a sampling sequence are no longer randomly picked, but rather they follow the constraint:\n",
    "$$ \\frac{T(x\\rightarrow x')}{T(x'\\rightarrow x)} = \\frac{w(x')}{w(x)} \\geq w_i,$$\n",
    "where $T(x\\rightarrow x')$ is the transition rate from state $x$ to $x',$ $w(x)$ the weight function, and $w_i$ a random number in [0,1].\n",
    "This equation implies *detailed balance* in statistical mechanics.\n",
    "If $w(x')/w(x) \\geq w_i$ we accept the new state $x'$, otherwise we reject it and use the old configuration $x$ to run another trial step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks about the implemented Metropolis algorithm:\n",
    "1. We first run the (dummy) Metropolis algorith for `nburnin` times so that the correlation with the chiuce of the initial state is reduced; this is sometimes called *burn-in* process.\n",
    "2. We do not sample the data points consecutively, because it would give rise to a high correlation error.\n",
    "Instead, we skip a few points in between each sampling.\n",
    "3. The accepting rate, i.e. the probability of a new configuration to be accepted, will be preferred to be in the vicinity of 50%.\n",
    "If the accepring rate is too high, the Metropolis algorithm behaves just like a basic importance sampling scheme.\n",
    "On the other hand, when the accepting rate is very low, much of the time is wasted in searching the new configuration.\n",
    "An accepting rate of around 50% usually offers a balanced performance in terms of efficiency and accuracy.\n",
    "The accepting rate is intimately dependent in the step size `h`, which is used to control the searching window for the new point.\n",
    "\n",
    "\n",
    "The improvement of the absolute error might not be straightforward if we compare the Metropolis scheme implemented in the class `metropolis` to the direct importance sampling methods.\n",
    "However, the error of a Metropolis integration should be much smaller.\n",
    "Bear in mind that we're using an elementary weight function here (same as the importance sampling), and is quite challenging to find a very good weight function.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>TODO:</b> Change the step size `h` and see how the accepting rate responds. Does a large `h` lead to a small accepting rate?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metropolis\n",
    "\n",
    "nskip = 1\n",
    "nburnin = nsize\n",
    "h = 0.9\n",
    "\n",
    "task = mc.metropolis(nsize, nskip, nburnin, h, verbose)\n",
    "task.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between data points can be analyzed by the *autocorrelation function*, which is defined as\n",
    "$$ C(l)=\\frac{\\left\\langle A_{n+l} A_n\\right\\rangle-\\left\\langle A_n\\right\\rangle^2}{\\left\\langle A_n^2\\right\\rangle-\\left\\langle A_n\\right\\rangle^2},$$\n",
    "where $ \\left\\langle A_{n+l} A_n\\right\\rangle=\\frac{1}{M} \\sum_{n=1}^M A_{n+l} A_n $ and $M$ is `nsize` in our code.\n",
    "\n",
    "You can check the correlation error from consecutive points by plotting the correlation function vs the lagging steps $l.$\n",
    "\n",
    "Find the minimum number of samplings $l$ we need to skip in order to get a minimal correlation beween $A_n$ and $A_{n+l}.$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>TODO:</b> Plot the autocorrelation function and determine the minimum value of `l` such that the correlation between two samplings is minimal.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autocorrelation\n",
    "\n",
    "# lmax = 20               # the largest l of the series\n",
    "# nrep = 10               # perform some independent experiments and do an ensemble-average \n",
    "lmax = 20\n",
    "nrep = 3\n",
    "corr = np.zeros((lmax + 1,2))       # create a two column array to store C(l) vs. l.\n",
    "corr[:,0] = np.arange(0,lmax+1,1)\n",
    "for i in range(lmax+1):\n",
    "   c = 0\n",
    "   for j in range(nrep):\n",
    "       task = mc.metropolis(nsize, i, nburnin, h, verbose)\n",
    "       task.run()\n",
    "       c += task.corr\n",
    "   corr[i,1] = c/nrep\n",
    "\n",
    "np.savetxt('autocorr.dat', corr)    # for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plot autocorrelation \"\"\"\n",
    "\n",
    "corr = np.loadtxt('autocorr.dat')\n",
    "plt.figure()\n",
    "plt.plot(corr[:,0], corr[:,1], marker='x', ls='')\n",
    "plt.xlabel('l')\n",
    "plt.ylabel('C(l)')\n",
    "plt.title('autocorrelation')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
